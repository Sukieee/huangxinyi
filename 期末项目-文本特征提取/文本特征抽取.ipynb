{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Thread-1Starting Thread-2Exiting Main Thread\n",
      "\n",
      "\n",
      "Thread-1: Wed Jan 29 15:03:20 2020\n",
      "Thread-2: Wed Jan 29 15:03:21 2020\n",
      "Thread-1: Wed Jan 29 15:03:21 2020\n",
      "Thread-1: Wed Jan 29 15:03:22 2020\n",
      "Thread-2: Wed Jan 29 15:03:23 2020\n",
      "Thread-1: Wed Jan 29 15:03:23 2020\n",
      "Thread-1: Wed Jan 29 15:03:24 2020\n",
      "Exiting Thread-1\n",
      "Thread-2: Wed Jan 29 15:03:25 2020\n",
      "Thread-2: Wed Jan 29 15:03:27 2020\n",
      "Thread-2: Wed Jan 29 15:03:29 2020\n",
      "Exiting Thread-2\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    " \n",
    "exitFlag = 0\n",
    " \n",
    "class myThread (threading.Thread):   #继承父类threading.Thread\n",
    "    def __init__(self, threadID, name, counter):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.counter = counter\n",
    "    def run(self):                   #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 \n",
    "        print (\"Starting \" + self.name)\n",
    "        print_time(self.name, self.counter, 5)\n",
    "        print (\"Exiting \" + self.name)\n",
    "        \n",
    "def print_time(threadName, delay, counter):\n",
    "    while counter:\n",
    "        if exitFlag:\n",
    "            (threading.Thread).exit()\n",
    "        time.sleep(delay)\n",
    "        print (\"%s: %s\" % (threadName, time.ctime(time.time())))\n",
    "        counter -= 1\n",
    " \n",
    "# 创建新线程\n",
    "thread1 = myThread(1, \"Thread-1\", 1)\n",
    "thread2 = myThread(2, \"Thread-2\", 2)\n",
    " \n",
    "# 开启线程\n",
    "thread1.start()\n",
    "thread2.start()\n",
    " \n",
    "print(\"Exiting Main Thread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Descriptive：Number of paragraphs (DESPC)\n",
    "\n",
    "#import re;\n",
    "#p=re.compile('\\n',re.S);\n",
    "#fileContent=open('test/test.txt','r').read();#读文件内容\n",
    "#paraList=p.split(fileContent) #根据换行符对文本进行切片\n",
    "#\n",
    "#print(\"The total number of paragraphs in the text is: \"+str(len(paraList)-1))#最后一个是空字符\n",
    "#\n",
    "#fileWriter=open('test/0.txt','a');#创建一个写文件的句柄\n",
    "#for paraIndex in range(len(paraList)):#遍历切片后的文本列表\n",
    "#    fileWriter.write(paraList[paraIndex]);#先将列表中第一个元素写入文件里\n",
    "#    fileWriter.close(); #关闭当前句柄\n",
    "#    fileWriter=open('test/'+str((paraIndex+1))+'.txt','a'); #又一次创建一个新的句柄。等待写入下一个切片元素。\n",
    "#        \n",
    "#fileWriter.close();#关闭最后创建的那个写文件句柄\n",
    "#print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of paragraphs in the text is: 42\n",
      "The total number of sentences in the text is: 117\n",
      "The average number of sentences in each paragraph within the text is: 2.7857142857142856\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def paragraphs(fileContent): #1.1 Descriptive：Number of paragraphs (DESPC)\n",
    "    p=re.compile('\\n',re.S)\n",
    "    paraList=p.split(fileContent) #根据换行符对文本进行切片\n",
    "    return len(paraList)-1\n",
    "\n",
    "def sentences(fileContent): #1.2 Descriptive：Number of sentences (DESSC)\n",
    "    p=re.compile('。',re.S) #以中文文本为例\n",
    "    paraList=p.split(fileContent) #根据句号对文本进行切片\n",
    "    return len(paraList)-1\n",
    "\n",
    "    \n",
    "    \n",
    "fileContent=open('test/test.txt','r').read() #读文件内容\n",
    "paragraphs(fileContent)\n",
    "sentences(fileContent)\n",
    "Mean_length(fileContent)\n",
    "print(\"The total number of paragraphs in the text is: \"+ str(paragraphs(fileContent)))\n",
    "print(\"The total number of sentences in the text is: \"+ str(sentences(fileContent)))\n",
    "\n",
    "#1.3 Descriptive：Mean length of paragraphs  (DESPL)\n",
    "print(\"The average number of sentences in each paragraph within the text is: \"+ str(sentences(fileContent)/paragraphs(fileContent))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator.CHAO-20180828JC/nltk_data'\n    - 'c:\\\\users\\\\administrator.chao-20180828jc\\\\appdata\\\\local\\\\programs\\\\python\\\\python37-32\\\\nltk_data'\n    - 'c:\\\\users\\\\administrator.chao-20180828jc\\\\appdata\\\\local\\\\programs\\\\python\\\\python37-32\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\administrator.chao-20180828jc\\\\appdata\\\\local\\\\programs\\\\python\\\\python37-32\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator.CHAO-20180828JC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-8bd207a72422>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mstopword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stop_words.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#停用词（虚词）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0menwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator.chao-20180828jc\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator.chao-20180828jc\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator.chao-20180828jc\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator.chao-20180828jc\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator.chao-20180828jc\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator.CHAO-20180828JC/nltk_data'\n    - 'c:\\\\users\\\\administrator.chao-20180828jc\\\\appdata\\\\local\\\\programs\\\\python\\\\python37-32\\\\nltk_data'\n    - 'c:\\\\users\\\\administrator.chao-20180828jc\\\\appdata\\\\local\\\\programs\\\\python\\\\python37-32\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\administrator.chao-20180828jc\\\\appdata\\\\local\\\\programs\\\\python\\\\python37-32\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator.CHAO-20180828JC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#2.1 Lexical Diversity：Type-token ratio: LDTTRc\n",
    "#查阅资料得知：频率为1的词称为type，LDTTRc = 实词type数/总词数\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "fileContent=open('test/test2.txt','r').read() #读文件内容，以英文文本为例\n",
    "str1=fileContent.split() #分词\n",
    "\n",
    "enword={}\n",
    "summ = len(str1) #总词频\n",
    "for w in str1:\n",
    "    if len(w)>1:\n",
    "        enword[w]=str1.count(w) #得到词频表\n",
    "\n",
    "stopword = open('stop_words.txt','r').read() #停用词（虚词）\n",
    "stopwords = nltk.word_tokenize(stopword)\n",
    "\n",
    "enwords = list(enword) - stopwords #得到实词\n",
    "dict(enwords)\n",
    "\n",
    "Type = 0\n",
    "for key,value in enwords.items():\n",
    "    if value == 1:\n",
    "        Type+=1\n",
    "#print(Type)\n",
    "\n",
    "TTR = (Type/summ)\n",
    "print(TTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4791666666666667\n"
     ]
    }
   ],
   "source": [
    "#2.2 Lexical Diversity：Type-token ratio: LDTTRa\n",
    "#查阅资料得知：频率为1的词称为type，LDTTRa = type的数量/总词数\n",
    "\n",
    "import re\n",
    "fileContent=open('test/test2.txt','r').read() #读文件内容，以英文文本为例\n",
    "str1=fileContent.split() #分词\n",
    "\n",
    "enword={}\n",
    "summ = len(str1) #总词频\n",
    "for w in str1:\n",
    "    if len(w)>1:\n",
    "        enword[w]=str1.count(w) #得到词频表\n",
    "#print(enword)\n",
    "\n",
    "Type = 0\n",
    "for key,value in enword.items():\n",
    "    if value == 1:\n",
    "        Type+=1\n",
    "#print(Type)\n",
    "\n",
    "TTR = Type/summ\n",
    "print(TTR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MTLD lexcical diversity measure for all words is :13.434318698618156\n"
     ]
    }
   ],
   "source": [
    "#2.3 Lexical Diversity：Type-token ratio: LDMTLDa\n",
    "#查阅https://blog.csdn.net/qq_36652619/article/details/77253208得知，传入一个词的list，MTLD就是计算词串的长度。\n",
    "\n",
    "#步骤一：计算MTLD过程中，由篇首开始依序算出文本中的每一个字汇的TTR值。当字汇的TTR值达到成小于0.72时，切割文本•文本至此称该字率为一个因子（factor);\n",
    "#下一个字再重新计算字汇TTR值，待TTR值达到成小于0.72时，再次切割文本，重新开始计算字汇TTR值并划分数个因子直到文本结束。\n",
    "#文本最后计算之因子并不完整，故需计算不完整因子分数以提高MTLD之可信度，其公式如下：IFS=(1-RS)/(1-0.72)。IFS为不完整因子数，RS为文本最后的TTR值\n",
    "\n",
    "#步骤二：将文本的总token数除以因子数与不完整因子分数之和，即算出MTLD。\n",
    "\n",
    "\n",
    "\n",
    "def MTLD(self,tokens):\n",
    "    # 判断list要够长，否则当前ttr的值没有效果\n",
    "    if len(tokens) < 10:\n",
    "        return -1\n",
    "    # 先得到一个type的list\n",
    "    types = []\n",
    "    factors = 0.0\n",
    "    now_ttr = 1.0\n",
    "    ttr_standard = 0.72\n",
    "    tokens_number = 0\n",
    "    types_number = 0\n",
    "    for gram in tokens:\n",
    "        gram = gram.lower()\n",
    "        tokens_number+=1\n",
    "        if gram not in types:\n",
    "            types_number+=1\n",
    "            types.append(gram)\n",
    "        now_ttr = types_number*1.0/tokens_number\n",
    "        if now_ttr < ttr_standard:\n",
    "            factors+=1.0\n",
    "            now_ttr = 1.0\n",
    "            tokens_number = 0\n",
    "            types_number = 0\n",
    "            types = []\n",
    "    RS = now_ttr\n",
    "    #print(now_ttr) \n",
    "    IFS = (1-RS)*1.0/0.28\n",
    "    IFS+=factors\n",
    "    if IFS != 0:\n",
    "        return len(tokens)/IFS\n",
    "\n",
    "print(\"The MTLD lexcical diversity measure for all words is :\" + str(MTLD(MTLD,fileContent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "#3.1 Readability：Flesch Reading Ease: RDFRE\n",
    "\n",
    "# READFRE = 206.835 - (1.015 x ASL) - (84.6 x ASW) \n",
    "# ASL=平均句子长度=单词的数量除以句子的数量，ASW=平均每个单词的音节数=音节数除以单词数\n",
    "\n",
    "import re\n",
    "\n",
    "fileContent=open('test/test2.txt','r').read() #读文件内容\n",
    "str1=fileContent.split()\n",
    "\n",
    "lis=\"\"\n",
    "for line in fileContent:\n",
    "    li=line.strip(\"\\n\")   \n",
    "    lis=lis+li\n",
    "    lis=lis+''    #在每个读取出来的字符后面加空格，分开每个单词\n",
    "\n",
    "liss=lis.split(\".\")\n",
    "sentences=0\n",
    "for h in liss:\n",
    "    if h == '':\n",
    "        continue\n",
    "    h.strip('')\n",
    "    h=h+'.' #在读出的每个句子的最后加上句号\n",
    "    #print(h)\n",
    "    sentences +=1\n",
    "\n",
    "ASL= len(str1)/(sentences-1)\n",
    "#print(ASL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_curses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4c4d688148a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcurses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascii\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misdigit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcmudict\u001b[0m \u001b[1;31m#此代码将使每个输入单词与cmu词典匹配。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator.chao-20180828jc\\appdata\\local\\programs\\python\\python37-32\\lib\\curses\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \"\"\"\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0m_curses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_curses'"
     ]
    }
   ],
   "source": [
    "#音节计算 https://www.icode9.com/content-1-589199.html。\n",
    "import re\n",
    "import nltk\n",
    "from curses.ascii import isdigit\n",
    "from nltk.corpus import cmudict #此代码将使每个输入单词与cmu词典匹配。\n",
    "import nltk.data\n",
    "import pprints\n",
    "\n",
    "d = cmudict.dict()\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "fp = open(\"test/test2.txt\")\n",
    "data = fp.read()\n",
    "tokens = nltk.wordpunct_tokenize(data)\n",
    "text = nltk.Text(tokens)\n",
    "words = [w.lower() for w in text]\n",
    "print (words) #to print all the words in input text\n",
    "regexp = \"[A-Za-z]+\"\n",
    "exp = re.compile(regexp)\n",
    "\n",
    "def nsyl(word):\n",
    "    return max([len([y for y in x if isdigit(y[-1])]) for x in d[word]])\n",
    "\n",
    "sum1 = 0\n",
    "count = 0\n",
    "for a in words:\n",
    "    if exp.match(a):\n",
    "        print (a)\n",
    "        print (\"no of syllables:\",nysl(a))\n",
    "        sum1 = sum1 + nysl(a)\n",
    "        print (\"sum of syllables:\",sum1)\n",
    "        count = count + 1\n",
    "\n",
    "ASW=count\n",
    "READFRE = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
    "print(\"The output of the Flesch Reading Ease is: \"+ str(READFRE))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ASL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e3d9effe27d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#3.2 Readability：Flesch_Kincaid Grade Level: RDFKGL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mREADFKGL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.39\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mASL\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m11.8\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mASW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m15.59\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The output of the Flesch-Kincaid Grade Level is: \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREADFKGL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ASL' is not defined"
     ]
    }
   ],
   "source": [
    "#3.2 Readability：Flesch_Kincaid Grade Level: RDFKGL\n",
    "\n",
    "READFKGL = (0.39 * ASL) + (11.8 * ASW) - 15.59 \n",
    "print(\"The output of the Flesch-Kincaid Grade Level is: \"+ str(READFKGL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3 Readability：RDL2\n",
    "\n",
    "#第二语言可读性得分,此项没有办法计算得出。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
